{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping using BeautifulSoup, Pandas, and Requests/Splinter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dependencies\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "\n",
    "import requests\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "\n",
    "from urls_list import * #where all urls and paths are saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pages = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Mars News\n",
    "\n",
    "* **Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that we can reference later.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_mars_news(no_pages=10):\n",
    "    #Scrape the latest news (pages = no_pages, default 10)\n",
    "    #If there are not enough pages, then scrape till end (based on More at the end)\n",
    "    #Return the unique news as list of dictionaries\n",
    "    \n",
    "    #Configure Browser\n",
    "    browser = Browser(browser_choice, executable_path=executable_path, headless=True)\n",
    "    #Visit url\n",
    "    browser.visit(nasa_mars_news)\n",
    "    page = 1\n",
    "    next_page = True\n",
    "    #Fieldnames to scrap\n",
    "    fields = ['date', 'news_title', 'news_p']\n",
    "    #Initialize news list\n",
    "    news_list = []\n",
    "    \n",
    "    while (page<=no_pages) and (next_page is not None):\n",
    "        #Create a bs object with lxml \n",
    "        soup = bs(browser.html, 'lxml')\n",
    "        try:\n",
    "            news = soup.find_all('li', class_='slide')\n",
    "            #Append the scraped data to the list\n",
    "            news_list += [{fields[index]:value.text for index,value in enumerate(news_entry.div.find(class_='list_text').find_all('div'))} for news_entry in news]\n",
    "            #increment\n",
    "            page+=1\n",
    "            #Check if \"more button\" is present\n",
    "            #This post is wow\n",
    "            #https://stackoverflow.com/questions/46468030/how-select-class-div-tag-in-splinter\n",
    "            next_page = browser.find_by_css('footer[class=\"list_footer more_button\"] a')\n",
    "            if next_page is not None:\n",
    "                time.sleep(2)#Delay\n",
    "                next_page.click()#Click More button\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            #Close browser to avoid resource issue (if the loop is finished)\n",
    "            if (page==no_pages) or (next_page is None):\n",
    "                browser.quit()\n",
    "            \n",
    "        #It looks like some of the pages have older data too. Hence, there are many duplicates.\n",
    "        #We need to remove duplicates\n",
    "    #Close browser to avoid resource issue\n",
    "    browser.quit()\n",
    "    \n",
    "    #Remove duplicates\n",
    "    unique_news_hash = set()\n",
    "    to_retain_index = []\n",
    "    for index, news in enumerate(news_list):\n",
    "        if news['date']+news['news_title'] not in unique_news_hash: \n",
    "            unique_news_hash.update({news['date']+news['news_title']}) #Update the hash if not present\n",
    "            to_retain_index.append(index) #Add the index to retain              \n",
    "\n",
    "\n",
    "    news_list = [news_list[Id] for Id in to_retain_index]\n",
    "    \n",
    "    return news_list\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "L  = scrape_mars_news(no_pages=no_pages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
